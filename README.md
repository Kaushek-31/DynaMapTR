DynaMapTR: Dynamic-Aware HD Map ConstructionUniversity of Michigan | ROB 535 Final ProjectThis repository contains the implementation of DynaMapTR, a dynamic-aware HD map construction pipeline. This method integrates BEVFormer and MapTRv2 with a novel segmentation-guided BEV masking block to explicitly suppress dynamic object activations (vehicles, pedestrians) before map decoding.ğŸ“– AbstractHigh-Definition (HD) maps are crucial for autonomous driving, but current BEV-based mapping methods (like MapTR) suffer from interference caused by dynamic objects. These objects create strong feature activations that degrade transformer attention and distort static geometric predictions.DynaMapTR addresses this by:Extracting BEV features using a BEVFormer backbone1111.Employing a lightweight segmentation head (SegEncodeV2) to detect dynamic objects directly in BEV space2222.Applying a segmentation-guided masking block to filter out dynamic features prior to the MapTR decoder3.This results in more stable attention and geometrically accurate vector maps, particularly in cluttered urban scenes4.ğŸ—ï¸ ArchitectureThe pipeline consists of three main stages5:BEV Encoder: Generates dense BEV features from multi-view images.Dynamic Masking Module: Performs 3-class segmentation (Background, Vehicle, Pedestrian) and masks the feature map6.Map Decoder: The masked features are passed to the MapTRv2 decoder to predict vector elements (lanes, dividers, crossings).(Refer to Fig 1. in the report for the architecture schematic) 7ğŸ› ï¸ InstallationPrerequisitesLinuxPython 3.8+PyTorch 1.10+CUDA 11.1+MapTRv2 DependenciesSetupBash# Clone the repository
git clone https://github.com/yourusername/DynaMapTR.git
cd DynaMapTR

# Install dependencies (Based on MapTR/mmdetection3d)
pip install -r requirements.txt
ğŸ“Š Dataset PreparationThis project uses the nuScenes dataset. Please download the dataset and organize it as follows:data/
  nuscenes/
    maps/
    samples/
    sweeps/
    v1.0-trainval/
Note: For dynamic object masking, we generate custom 3-class BEV segmentation labels derived from the annotated 3D bounding boxes8.ğŸš€ Training StrategyDynaMapTR utilizes a two-stage training strategy to prevent gradient interference between the segmentation and mapping tasks9.Stage 1: Segmentation TrainingIn this stage, we train the BEVFormer encoder and the SegEncodeV2 head jointly. The MapTR decoder is disabled10.Bash# Train the segmentation head and encoder
python tools/train.py configs/dynamaptr/stage1_segmentation.py
Objective: Minimize FocalLoss + DiceLoss11.Target: Background, Vehicles, Pedestrians.Stage 2: HD Map TrainingWe freeze the segmentation module to act as a stable filter. The masked BEV features are passed to the MapTR decoder12.Bash# Train the map decoder with frozen segmentation weights
python tools/train.py configs/dynamaptr/stage2_map_decoding.py --load_from work_dirs/stage1/latest.pth
ğŸ“ˆ ResultsSegmentation Performance (Stage 1)The lightweight segmentation module achieves the following IoU in BEV space13:ClassIoUBackground0.98Vehicles0.65Pedestrians0.10Map Construction QualityQualitative results demonstrate that DynaMapTR reduces "attention drift" around vehicles and produces clearer lane boundaries compared to the baseline MapTRv214141414.(Comparison of Ground Truth, Plain BEV, and Masked BEV predictions - see Fig. 3 in report) 15ğŸ‘¥ TeamKaushek Kumar T R - EECS Department - kaushek@umich.edu 16161616Maithreyan Ganesh - Robotics Department - maithgan@umich.edu 17171717Shivam Udeshi - EECS Department - sudeshi@umich.edu 18181818University of Michigan, Ann Arbor 19ğŸ™ AcknowledgementsWe thank the ROB 535 instructors for their guidance and compute resources20.This project builds upon excellent open-source work:MapTR / MapTRv2BEVFormer
